{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d9db61",
   "metadata": {},
   "source": [
    "This botebook uses the data generated by gen_daily_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d28db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark 3.5.4 http://DESKTOP-4GOMK6M:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SvnLocalSpark\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"../delta-data-tmp\")\\\n",
    "    .config(\"spark.jars.packages\",\"io.delta:delta-spark_2.13:3.3.0\")\\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .master(\"local\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"spark {spark.version} {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9456013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"../delta-data-tmp/integration.db/property_test1\"\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(folder_path):\n",
    "    # Delete the folder and all its contents\n",
    "    shutil.rmtree(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f6e3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[valid_from: date, valid_to: date, property_id: string, street: string, street_number: int, city: string, zip_code: int, category: string, property_value: double, energy_label: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "import json\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS integration\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS integration.property_test1\")\n",
    "\n",
    "# JSON string representing the schema (with metadata field added)\n",
    "json_schema = '''\n",
    "{\n",
    "    \"fields\":[\n",
    "        {\"metadata\":{},\"name\":\"valid_from\",\"nullable\":false,\"type\":\"date\"},\n",
    "        {\"metadata\":{},\"name\":\"valid_to\",\"nullable\":false,\"type\":\"date\"},\n",
    "        {\"metadata\":{},\"name\":\"property_id\",\"nullable\":false,\"type\":\"string\"},\n",
    "        {\"metadata\":{},\"name\":\"street\",\"nullable\":true,\"type\":\"string\"},\n",
    "        {\"metadata\":{},\"name\":\"street_number\",\"nullable\":true,\"type\":\"integer\"},\n",
    "        {\"metadata\":{},\"name\":\"city\",\"nullable\":true,\"type\":\"string\"},\n",
    "        {\"metadata\":{},\"name\":\"zip_code\",\"nullable\":true,\"type\":\"integer\"},\n",
    "        {\"metadata\":{},\"name\":\"category\",\"nullable\":true,\"type\":\"string\"},\n",
    "        {\"metadata\":{},\"name\":\"property_value\",\"nullable\":true,\"type\":\"double\"},\n",
    "        {\"metadata\":{},\"name\":\"energy_label\",\"nullable\":true,\"type\":\"string\"}\n",
    "    ],\n",
    "    \"type\":\"struct\"}\n",
    "'''\n",
    "\n",
    "# Deserialize the JSON string into a StructType schema\n",
    "schema_dict = json.loads(json_schema)\n",
    "schema = StructType.fromJson(schema_dict)\n",
    "\n",
    "# Create an empty DataFrame using the schema\n",
    "empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "spark.catalog.createTable(\"integration.property_test1\", source=\"delta\", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f560c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../resources/generated/commercial_property/2022/01/commercial_property_snapshot_20220101.csv\n",
      "../resources/generated/commercial_property/2022/01/commercial_property_snapshot_20220102.csv\n",
      "../resources/generated/commercial_property/2022/01/commercial_property_snapshot_20220103.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import lit,sha2,concat\n",
    "\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2022-01-03'\n",
    "\n",
    "\n",
    "# Convert dates to datetime objects\n",
    "start_date = datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "end_date = datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "max_date = datetime.strptime(\"9999-12-31\", '%Y-%m-%d').date()\n",
    "\n",
    "key_cols = [\"property_id\"]\n",
    "tl_cols = [\"valid_from\", \"valid_to\"]\n",
    "merge_cols = key_cols + [\"valid_from\"]\n",
    "data_cols = [c for c in spark.table(\"integration.property_test1\").columns if c not in (key_cols + tl_cols)]\n",
    "\n",
    "merge_tgt = DeltaTable.forName(spark,\"integration.property_test1\")\n",
    "\n",
    "current_date = start_date\n",
    "\n",
    "# get the new data\n",
    "while current_date <= end_date:\n",
    "    file_path = f\"../resources/generated/commercial_property/{current_date.strftime('%Y')}/{current_date.strftime('%m')}/commercial_property_snapshot_{current_date.strftime('%Y%m%d')}.csv\"\n",
    "    print(file_path)\n",
    "    inp = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\\\n",
    "        .withColumn(\"hash\",sha2(concat(*data_cols),512))\\\n",
    "        .join(spark.table(\"integration.property_test1\")\\\n",
    "              .withColumn(\"hash\",sha2(concat(*data_cols),512))\\\n",
    "            , on=(key_cols + [\"hash\"]), how = \"left_anti\")\\\n",
    "        .withColumn(\"valid_from\", lit(current_date))\\\n",
    "        .withColumn(\"valid_to\", lit(max_date))\\\n",
    "        .drop(\"hash\")\n",
    "\n",
    "    upd = spark.table(\"integration.property_test1\").where(\"valid_to=date '9999-12-31'\")\\\n",
    "        .withColumn(\"valid_to\", lit(current_date))\\\n",
    "        .join(inp,on=key_cols,how=\"left_semi\")\\\n",
    "        .unionByName(inp)\n",
    "    \n",
    "    merge_tgt.alias(\"tgt\").merge(upd.alias(\"src\")\n",
    "        ,\" and \".join([f\"tgt.{c}=src.{c}\" for c in merge_cols]))\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "\n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2606a3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-----------------+-------------+------------+--------+---------+--------------+------------+\n",
      "|valid_from|  valid_to|property_id|           street|street_number|        city|zip_code| category|property_value|energy_label|\n",
      "+----------+----------+-----------+-----------------+-------------+------------+--------+---------+--------------+------------+\n",
      "|2022-01-01|9999-12-31|       P001|    Poplar Street|          388|Fayetteville|   27505| Workshop|     109568.45|           A|\n",
      "|2022-01-01|9999-12-31|       P002|     Maple Street|          401|Indian Trail|   27572|   Office|     381282.69|           F|\n",
      "|2022-01-01|9999-12-31|       P003|   Asheville Road|          162|     Raleigh|   28727| Workshop|     217940.83|           F|\n",
      "|2022-01-01|9999-12-31|       P004|Greensboro Street|          563|     Sanford|   28881| Workshop|     395346.61|           A|\n",
      "|2022-01-01|9999-12-31|       P005|  Magnolia Street|          776|Fayetteville|   28872|Warehouse|     267624.86|           F|\n",
      "|2022-01-01|9999-12-31|       P006|    Willow Street|           21| Mooresville|   27805|   Office|      138142.3|           C|\n",
      "|2022-01-01|2022-01-02|       P007|      Meadow Lane|          978|     Concord|   28851|   Office|     423959.63|           D|\n",
      "|2022-01-02|9999-12-31|       P007|      Meadow Lane|          978|     Concord|   28851|   Office|     417086.44|           D|\n",
      "|2022-01-01|9999-12-31|       P008|   Tar Heel Drive|          849|  Kannapolis|   28083|   Office|     276441.41|           D|\n",
      "|2022-01-01|9999-12-31|       P009|      Park Avenue|          341|      Durham|   28625| Workshop|     313699.28|           B|\n",
      "|2022-01-01|9999-12-31|       P010|Greensboro Street|          639|  High Point|   27039|     Shop|     478367.12|           B|\n",
      "|2022-01-01|9999-12-31|       P011|       Ash Street|          923|      Durham|   28440|   Office|      471212.0|           A|\n",
      "|2022-01-01|9999-12-31|       P012|     Forest Drive|          112|     Concord|   28903|     Shop|     361169.69|           D|\n",
      "|2022-01-01|9999-12-31|       P013| Rocky Mount Road|          889|  High Point|   27170|   Office|     119095.36|           F|\n",
      "|2022-01-01|9999-12-31|       P014|      Park Avenue|          166|     Concord|   28967|     Shop|     233515.46|           B|\n",
      "|2022-01-01|9999-12-31|       P015|  Gastonia Street|          668|  Wilmington|   28097|   Office|     413599.61|           F|\n",
      "|2022-01-01|9999-12-31|       P016|       Ridge Road|          865|Indian Trail|   27098| Workshop|     239905.35|           A|\n",
      "|2022-01-01|9999-12-31|       P017|   Hickory Street|          648|     Sanford|   28762|Warehouse|     203506.52|           F|\n",
      "|2022-01-01|9999-12-31|       P018|Greensboro Street|          641|     Sanford|   28786|Warehouse|     402877.12|           B|\n",
      "|2022-01-01|9999-12-31|       P019|      Pine Street|          609|  High Point|   27495|Warehouse|     191700.02|           E|\n",
      "+----------+----------+-----------+-----------------+-------------+------------+--------+---------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"integration.property_test1\").orderBy(\"property_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff74ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec04e497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">version</td><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">userId</td><td style=\"font-weight: bold\">userName</td><td style=\"font-weight: bold\">operation</td><td style=\"font-weight: bold\">operationParameters</td><td style=\"font-weight: bold\">job</td><td style=\"font-weight: bold\">notebook</td><td style=\"font-weight: bold\">clusterId</td><td style=\"font-weight: bold\">readVersion</td><td style=\"font-weight: bold\">isolationLevel</td><td style=\"font-weight: bold\">isBlindAppend</td><td style=\"font-weight: bold\">operationMetrics</td><td style=\"font-weight: bold\">userMetadata</td><td style=\"font-weight: bold\">engineInfo</td></tr><tr><td>3</td><td>2025-04-14 13:26:35.707000</td><td>null</td><td>null</td><td>MERGE</td><td>{&#x27;matchedPredicates&#x27;: &#x27;[{&quot;actionType&quot;:&quot;update&quot;}]&#x27;, &#x27;predicate&#x27;: &#x27;[&quot;((property_id#193 = property_id#4158) AND (valid_from#191 = valid_from#4156))&quot;]&#x27;, &#x27;notMatchedBySourcePredicates&#x27;: &#x27;[]&#x27;, &#x27;notMatchedPredicates&#x27;: &#x27;[{&quot;actionType&quot;:&quot;insert&quot;}]&#x27;}</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Serializable</td><td>False</td><td>{&#x27;numOutputRows&#x27;: &#x27;106&#x27;, &#x27;numTargetBytesAdded&#x27;: &#x27;6039&#x27;, &#x27;numTargetRowsInserted&#x27;: &#x27;3&#x27;, &#x27;numTargetRowsMatchedDeleted&#x27;: &#x27;0&#x27;, &#x27;numTargetFilesAdded&#x27;: &#x27;1&#x27;, &#x27;materializeSourceTimeMs&#x27;: &#x27;1088&#x27;, &#x27;numTargetFilesRemoved&#x27;: &#x27;1&#x27;, &#x27;numTargetRowsMatchedUpdated&#x27;: &#x27;3&#x27;, &#x27;executionTimeMs&#x27;: &#x27;2778&#x27;, &#x27;numTargetDeletionVectorsUpdated&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsCopied&#x27;: &#x27;100&#x27;, &#x27;rewriteTimeMs&#x27;: &#x27;303&#x27;, &#x27;numTargetRowsUpdated&#x27;: &#x27;3&#x27;, &#x27;numTargetDeletionVectorsRemoved&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsDeleted&#x27;: &#x27;0&#x27;, &#x27;scanTimeMs&#x27;: &#x27;1381&#x27;, &#x27;numSourceRows&#x27;: &#x27;6&#x27;, &#x27;numTargetDeletionVectorsAdded&#x27;: &#x27;0&#x27;, &#x27;numTargetChangeFilesAdded&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsNotMatchedBySourceUpdated&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsNotMatchedBySourceDeleted&#x27;: &#x27;0&#x27;, &#x27;numTargetBytesRemoved&#x27;: &#x27;5972&#x27;}</td><td>null</td><td>Apache-Spark/3.5.4 Delta-Lake/3.3.0</td></tr><tr><td>2</td><td>2025-04-14 13:26:30.866000</td><td>null</td><td>null</td><td>MERGE</td><td>{&#x27;matchedPredicates&#x27;: &#x27;[{&quot;actionType&quot;:&quot;update&quot;}]&#x27;, &#x27;predicate&#x27;: &#x27;[&quot;((property_id#193 = property_id#2268) AND (valid_from#191 = valid_from#2266))&quot;]&#x27;, &#x27;notMatchedBySourcePredicates&#x27;: &#x27;[]&#x27;, &#x27;notMatchedPredicates&#x27;: &#x27;[{&quot;actionType&quot;:&quot;insert&quot;}]&#x27;}</td><td>null</td><td>null</td><td>null</td><td>1</td><td>Serializable</td><td>False</td><td>{&#x27;numOutputRows&#x27;: &#x27;103&#x27;, &#x27;numTargetBytesAdded&#x27;: &#x27;5972&#x27;, &#x27;numTargetRowsInserted&#x27;: &#x27;3&#x27;, &#x27;numTargetRowsMatchedDeleted&#x27;: &#x27;0&#x27;, &#x27;numTargetFilesAdded&#x27;: &#x27;1&#x27;, &#x27;materializeSourceTimeMs&#x27;: &#x27;1339&#x27;, &#x27;numTargetFilesRemoved&#x27;: &#x27;1&#x27;, &#x27;numTargetRowsMatchedUpdated&#x27;: &#x27;3&#x27;, &#x27;executionTimeMs&#x27;: &#x27;3544&#x27;, &#x27;numTargetDeletionVectorsUpdated&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsCopied&#x27;: &#x27;97&#x27;, &#x27;rewriteTimeMs&#x27;: &#x27;541&#x27;, &#x27;numTargetRowsUpdated&#x27;: &#x27;3&#x27;, &#x27;numTargetDeletionVectorsRemoved&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsDeleted&#x27;: &#x27;0&#x27;, &#x27;scanTimeMs&#x27;: &#x27;1656&#x27;, &#x27;numSourceRows&#x27;: &#x27;6&#x27;, &#x27;numTargetDeletionVectorsAdded&#x27;: &#x27;0&#x27;, &#x27;numTargetChangeFilesAdded&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsNotMatchedBySourceUpdated&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsNotMatchedBySourceDeleted&#x27;: &#x27;0&#x27;, &#x27;numTargetBytesRemoved&#x27;: &#x27;5911&#x27;}</td><td>null</td><td>Apache-Spark/3.5.4 Delta-Lake/3.3.0</td></tr><tr><td>1</td><td>2025-04-14 13:26:25.174000</td><td>null</td><td>null</td><td>MERGE</td><td>{&#x27;matchedPredicates&#x27;: &#x27;[{&quot;actionType&quot;:&quot;update&quot;}]&#x27;, &#x27;predicate&#x27;: &#x27;[&quot;((property_id#193 = property_id#332) AND (valid_from#191 = valid_from#330))&quot;]&#x27;, &#x27;notMatchedBySourcePredicates&#x27;: &#x27;[]&#x27;, &#x27;notMatchedPredicates&#x27;: &#x27;[{&quot;actionType&quot;:&quot;insert&quot;}]&#x27;}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>False</td><td>{&#x27;numOutputRows&#x27;: &#x27;100&#x27;, &#x27;numTargetBytesAdded&#x27;: &#x27;5911&#x27;, &#x27;numTargetRowsInserted&#x27;: &#x27;100&#x27;, &#x27;numTargetRowsMatchedDeleted&#x27;: &#x27;0&#x27;, &#x27;numTargetFilesAdded&#x27;: &#x27;1&#x27;, &#x27;materializeSourceTimeMs&#x27;: &#x27;6102&#x27;, &#x27;numTargetFilesRemoved&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsMatchedUpdated&#x27;: &#x27;0&#x27;, &#x27;executionTimeMs&#x27;: &#x27;9924&#x27;, &#x27;numTargetDeletionVectorsUpdated&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsCopied&#x27;: &#x27;0&#x27;, &#x27;rewriteTimeMs&#x27;: &#x27;1038&#x27;, &#x27;numTargetRowsUpdated&#x27;: &#x27;0&#x27;, &#x27;numTargetDeletionVectorsRemoved&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsDeleted&#x27;: &#x27;0&#x27;, &#x27;scanTimeMs&#x27;: &#x27;2756&#x27;, &#x27;numSourceRows&#x27;: &#x27;100&#x27;, &#x27;numTargetDeletionVectorsAdded&#x27;: &#x27;0&#x27;, &#x27;numTargetChangeFilesAdded&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsNotMatchedBySourceUpdated&#x27;: &#x27;0&#x27;, &#x27;numTargetRowsNotMatchedBySourceDeleted&#x27;: &#x27;0&#x27;, &#x27;numTargetBytesRemoved&#x27;: &#x27;0&#x27;}</td><td>null</td><td>Apache-Spark/3.5.4 Delta-Lake/3.3.0</td></tr><tr><td>0</td><td>2025-04-14 13:26:09.895000</td><td>null</td><td>null</td><td>CREATE TABLE</td><td>{&#x27;partitionBy&#x27;: &#x27;[]&#x27;, &#x27;description&#x27;: None, &#x27;properties&#x27;: &#x27;{}&#x27;, &#x27;clusterBy&#x27;: &#x27;[]&#x27;, &#x27;isManaged&#x27;: &#x27;true&#x27;}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>True</td><td>{}</td><td>null</td><td>Apache-Spark/3.5.4 Delta-Lake/3.3.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "DESCRIBE HISTORY integration.property_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953b30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
