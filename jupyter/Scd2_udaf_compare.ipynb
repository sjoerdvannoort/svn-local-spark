{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972820a0",
   "metadata": {},
   "source": [
    "This example reads snapshot data and creates a slowly changing dimension from it using hashing and window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37407819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark 3.5.4 http://DESKTOP-4GOMK6M:4041\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SvnLocalSpark\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"../spark-data-tmp\")\\\n",
    "    .config(\"spark.jars\", \"../scala-udaf/target/scala-2.13/svn-local-spark_2.13-0.1.0-SNAPSHOT.jar\") \\\n",
    "    .master(\"local\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"spark {spark.version} {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac3afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS landing\")\n",
    "# create external table\n",
    "spark.catalog.getDatabase(\"landing\")\n",
    "spark.catalog.createTable(\n",
    "    tableName = \"landing.commercial_properties\",\n",
    "    source = \"csv\",\n",
    "    description = \"property values\",\n",
    "    header=\"true\", delimiter=\",\", path=\"../../resources/sourcedata/commercial_property_snapshots_100_M39.csv\", inferSchema=\"true\")\n",
    "\n",
    "raw = spark.table(\"landing.commercial_properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f82313",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_catalyst_udf = spark.sparkContext._jvm.spark.udaf.SvnFunctionRegistration.registerFunctions(spark._jsparkSession)\n",
    "java_udf1 = spark.sparkContext._jvm.spark.udaf.LeadUnequalDateString.register(spark._jsparkSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d95b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "snapshot_colname= \"date\"\n",
    "from_colname = \"valid_from\"\n",
    "to_colname = \"valid_to\"\n",
    "key_cols = [\"property_id\"]\n",
    "inp_cols = key_cols +[snapshot_colname]\n",
    "data_cols = [c for c in raw.columns if c not in inp_cols ]\n",
    "\n",
    "sdw = Window.partitionBy(key_cols).orderBy(snapshot_colname)\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS integration\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df544c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scdv1 =  raw.withColumn(\"hash\",sha2(concat(*data_cols),512))\\\n",
    "            .withColumn(\"prevHash\",lag(\"hash\").over(sdw))\\\n",
    "            .where(\"hash<>prevHash or prevHash IS NULL\")\\\n",
    "            .withColumn(to_colname, coalesce(lead(snapshot_colname).over(sdw), to_date(lit(\"9999-12-31\"))))\\\n",
    "            .select(key_cols +[col(snapshot_colname).alias(from_colname)] +[to_colname] + data_cols)\n",
    "\n",
    "scdv1.write.mode('overwrite').format(\"parquet\").saveAsTable(\"integration.properties_lead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf92d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "scdv2 =  raw.withColumn(\"hash\",sha2(concat(*data_cols),512))\\\n",
    "            .withColumn(\"prevHash\",lag(\"hash\").over(sdw))\\\n",
    "            .withColumn(to_colname, expr('coalesce(lead_unequal(`date`,`hash`) over(partition by property_id ORDER BY `date`), to_date(\"9999-12-31\"))'))\\\n",
    "            .where(\"hash<>prevHash or prevHash IS NULL\")\\\n",
    "            .select(key_cols +[col(snapshot_colname).alias(from_colname)] +[to_colname] + data_cols)\n",
    "\n",
    "scdv2.write.mode('overwrite').format(\"parquet\").saveAsTable(\"integration.properties_lead_uneqal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b02e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scdv3 =  raw.withColumn(\"hash\",sha2(concat(*data_cols),512))\\\n",
    "            .withColumn(\"prevHash\",lag(\"hash\").over(sdw))\\\n",
    "            .withColumn(to_colname, expr('coalesce(lead_unequal_date_string(`date`,`hash`) over(partition by property_id ORDER BY `date`  ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING), to_date(\"9999-12-31\"))'))\\\n",
    "            .where(\"hash<>prevHash or prevHash IS NULL\")\\\n",
    "            .select(key_cols +[col(snapshot_colname).alias(from_colname)] +[to_colname] + data_cols)\n",
    "\n",
    "scdv3.write.mode('overwrite').format(\"parquet\").saveAsTable(\"integration.properties_lead_uneqal_ds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
