{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d9db61",
   "metadata": {},
   "source": [
    "This botebook uses the data generated by gen_daily_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d28db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark 3.5.4 http://DESKTOP-4GOMK6M:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SvnLocalSpark\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"../delta-data-tmp\")\\\n",
    "    .config(\"spark.jars.packages\",\"io.delta:delta-spark_2.13:3.3.0\")\\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .master(\"local\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"spark {spark.version} {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9456013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"../delta-data-tmp/append_test1\"\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(folder_path):\n",
    "    # Delete the folder and all its contents\n",
    "    shutil.rmtree(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f560c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../resources/generated/commercial_property/2022/01/commercial_property_snapshot_20220101.csv\n",
      "../resources/generated/commercial_property/2022/01/commercial_property_snapshot_20220102.csv\n",
      "../resources/generated/commercial_property/2022/01/commercial_property_snapshot_20220103.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import lit,sha2,concat,col\n",
    "\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2022-01-03'\n",
    "\n",
    "\n",
    "# Convert dates to datetime objects\n",
    "start_date = datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "end_date = datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "max_date = datetime.strptime(\"9999-12-31\", '%Y-%m-%d').date()\n",
    "\n",
    "current_date = start_date\n",
    "\n",
    "# get the new data\n",
    "while current_date <= end_date:\n",
    "    file_path = f\"../resources/generated/commercial_property/{current_date.strftime('%Y')}/{current_date.strftime('%m')}/commercial_property_snapshot_{current_date.strftime('%Y%m%d')}.csv\"\n",
    "    print(file_path)\n",
    "    inp = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\\\n",
    "        .withColumn(\"snapshot_date\", lit(current_date))\\\n",
    "        .withColumn(\"yyyy\", lit(datetime.strftime(current_date, \"%Y\")))\\\n",
    "        .withColumn(\"mm\", lit(datetime.strftime(current_date, \"%m\")))\\\n",
    "        .withColumn(\"dd\", lit(datetime.strftime(current_date, \"%d\")))\n",
    "\n",
    "    if DeltaTable.isDeltaTable(spark, folder_path):\n",
    "        tbl = DeltaTable.forPath(spark, folder_path)\n",
    "        tbl.delete(col(\"snapshot_date\")==current_date)\n",
    "    inp.write.format('delta').partitionBy(\"yyyy\", \"mm\", \"dd\").mode('append').save(\"../delta-data-tmp/append_test1\")\n",
    "\n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2606a3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "+-----------+-----------------+-------------+------------+--------+---------+--------------+------------+-------------+----+---+---+\n",
      "|property_id|           street|street_number|        city|zip_code| category|property_value|energy_label|snapshot_date|yyyy| mm| dd|\n",
      "+-----------+-----------------+-------------+------------+--------+---------+--------------+------------+-------------+----+---+---+\n",
      "|       P001|    Poplar Street|          388|Fayetteville|   27505| Workshop|     109568.45|           A|   2022-01-01|2022| 01| 01|\n",
      "|       P001|    Poplar Street|          388|Fayetteville|   27505| Workshop|     109568.45|           A|   2022-01-02|2022| 01| 02|\n",
      "|       P001|    Poplar Street|          388|Fayetteville|   27505| Workshop|     109568.45|           A|   2022-01-03|2022| 01| 03|\n",
      "|       P002|     Maple Street|          401|Indian Trail|   27572|   Office|     381282.69|           F|   2022-01-01|2022| 01| 01|\n",
      "|       P002|     Maple Street|          401|Indian Trail|   27572|   Office|     381282.69|           F|   2022-01-02|2022| 01| 02|\n",
      "|       P002|     Maple Street|          401|Indian Trail|   27572|   Office|     381282.69|           F|   2022-01-03|2022| 01| 03|\n",
      "|       P003|   Asheville Road|          162|     Raleigh|   28727| Workshop|     217940.83|           F|   2022-01-01|2022| 01| 01|\n",
      "|       P003|   Asheville Road|          162|     Raleigh|   28727| Workshop|     217940.83|           F|   2022-01-02|2022| 01| 02|\n",
      "|       P003|   Asheville Road|          162|     Raleigh|   28727| Workshop|     217940.83|           F|   2022-01-03|2022| 01| 03|\n",
      "|       P004|Greensboro Street|          563|     Sanford|   28881| Workshop|     395346.61|           A|   2022-01-01|2022| 01| 01|\n",
      "|       P004|Greensboro Street|          563|     Sanford|   28881| Workshop|     395346.61|           A|   2022-01-02|2022| 01| 02|\n",
      "|       P004|Greensboro Street|          563|     Sanford|   28881| Workshop|     395346.61|           A|   2022-01-03|2022| 01| 03|\n",
      "|       P005|  Magnolia Street|          776|Fayetteville|   28872|Warehouse|     267624.86|           F|   2022-01-01|2022| 01| 01|\n",
      "|       P005|  Magnolia Street|          776|Fayetteville|   28872|Warehouse|     267624.86|           F|   2022-01-02|2022| 01| 02|\n",
      "|       P005|  Magnolia Street|          776|Fayetteville|   28872|Warehouse|     267624.86|           F|   2022-01-03|2022| 01| 03|\n",
      "|       P006|    Willow Street|           21| Mooresville|   27805|   Office|      138142.3|           C|   2022-01-01|2022| 01| 01|\n",
      "|       P006|    Willow Street|           21| Mooresville|   27805|   Office|      138142.3|           C|   2022-01-02|2022| 01| 02|\n",
      "|       P006|    Willow Street|           21| Mooresville|   27805|   Office|      138142.3|           C|   2022-01-03|2022| 01| 03|\n",
      "|       P007|      Meadow Lane|          978|     Concord|   28851|   Office|     423959.63|           D|   2022-01-01|2022| 01| 01|\n",
      "|       P007|      Meadow Lane|          978|     Concord|   28851|   Office|     417086.44|           D|   2022-01-02|2022| 01| 02|\n",
      "+-----------+-----------------+-------------+------------+--------+---------+--------------+------------+-------------+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"../delta-data-tmp/append_test1\")\n",
    "print(df.count())\n",
    "df.orderBy(\"property_id\", \"snapshot_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "DESCRIBE HISTORY integration.property_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dbc282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "th = spark.sql(\"DESCRIBE HISTORY integration.property_test1\")\n",
    "th.select(\"version\", col(\"timestamp\"), \"operation\"\n",
    "          , col(\"operationMetrics.numTargetRowsInserted\").alias(\"RowsInserted\")\n",
    "          , col(\"operationMetrics.numTargetRowsUpdated\").alias(\"RowsUpdated\")\n",
    "          , \"operationMetrics.numSourceRows\", \"operationMetrics.executionTimeMs\"\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_property = spark.table(\"integration.property_test1\").where(\"valid_from>date '2022-01-01'\").select(\"property_id\").limit(1).collect()[0][0]\n",
    "#spark.table(\"integration.property_test1\").option(\"versionAsOf\", \"1\").where(\"valid_to=date '9999-12-31'\").where(f\"property_id='{example_property}'\").show()\n",
    "spark.sql(f\"SELECT * FROM integration.property_test1 WHERE valid_to=date '9999-12-31' AND property_id='{example_property}'\").show()\n",
    "spark.sql(f\"SELECT * FROM integration.property_test1 VERSION AS OF 1 WHERE valid_to=date '9999-12-31' AND property_id='{example_property}'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DeltaTable.forName(spark,\"integration.property_test1\")\n",
    "dt.history().select(\"version\", col(\"timestamp\"), \"operation\"\n",
    "          , col(\"operationMetrics.numTargetRowsInserted\").alias(\"RowsInserted\")\n",
    "          , col(\"operationMetrics.numTargetRowsUpdated\").alias(\"RowsUpdated\")\n",
    "          , \"operationMetrics.numSourceRows\", \"operationMetrics.executionTimeMs\"\n",
    "          ).show()\n",
    "print(\"compaction\")\n",
    "dt.optimize().executeCompaction().show()\n",
    "dt.history().select(\"version\", col(\"timestamp\"), \"operation\"\n",
    "          , col(\"operationMetrics.numTargetRowsInserted\").alias(\"RowsInserted\")\n",
    "          , col(\"operationMetrics.numTargetRowsUpdated\").alias(\"RowsUpdated\")\n",
    "          , \"operationMetrics.numSourceRows\", \"operationMetrics.executionTimeMs\"\n",
    "          ).show()\n",
    "print(\"ZOrder\")\n",
    "dt.optimize().executeZOrderBy(\"property_id\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
